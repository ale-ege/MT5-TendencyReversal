import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import joblib
import MetaTrader5 as mt5
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.metrics import accuracy_score
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.inspection import permutation_importance
from config import symbols, spread, valor_por_trade, alavancagem, capital_inicial, timeframe, num_candles, frame_period, N, threshold, importance_threshold, base_path, input_data
from data_mt5 import fetch_and_process_data
from funcoes import plot_importance, testar_thresholds_importancia
import sys
import talib
import matplotlib
# Inicializando resultados finais
resultados_finais = []

print(talib.__version__)  # Deve retornar a vers√£o (ex: 0.4.24)



def processar_acao(symbol, timeframe, num_candles=300, frame_period=300):
    """
    Processa os dados de um ativo financeiro espec√≠fico, coleta dados do MetaTrader 5,
    cria diret√≥rios para salvar os resultados e chama fun√ß√µes de processamento.

    Par√¢metros:
        symbol (str): O s√≠mbolo do ativo financeiro (exemplo: 'PETR4', 'AAPL').
    """

    # =============================================================================
    # 1. Processamento
    # =============================================================================

    # Exibe uma mensagem indicando que o processamento do ativo foi iniciado.
    print(f"\nIniciando processamento de {symbol}...")

    # Backend n√£o gr√°fico para evitar erros com Tkinter, importante para execu√ß√£o em servidores ou sem interface gr√°fica.

    matplotlib.use('Agg')  # Define o backend de matplotlib para n√£o precisar de interface gr√°fica (√∫til em servidores sem GUI)
    import matplotlib.pyplot as plt  # Importa o matplotlib para criar gr√°ficos (mesmo sem a GUI)

    # Configura√ß√£o do Pandas para exibir todas as linhas de um DataFrame sem trunc√°-las.
    pd.set_option('display.max_rows', None)  # Define que o Pandas deve exibir todas as linhas de um DataFrame

    # Define o caminho da pasta Train e cria o diret√≥rio para o s√≠mbolo e timeframe
    symbol_timeframe_path = os.path.join(base_path, f"{symbol}_{timeframe}")

    # Verifica se o diret√≥rio j√° existe, caso contr√°rio, cria-o
    if not os.path.exists(symbol_timeframe_path):
        os.makedirs(symbol_timeframe_path)
        print(f"Diret√≥rio criado para {symbol} e timeframe {timeframe}")

    # Chama a fun√ß√£o 'fetch_and_process_data' para coletar e processar os dados financeiros do ativo
    fetch_and_process_data(symbol, timeframe, num_candles, frame_period, base_path)


    # =============================================================================
    # 2. Carregar os Dados Originais df
    # =============================================================================
    print("Carregando dados do arquivo...")

    # Define o caminho para o arquivo de dados usando a vari√°vel input_data
    file_original = os.path.join(input_data, f"{symbol}_{timeframe}_data.csv")

    # Verifica se o arquivo existe e carrega os dados
    if os.path.exists(file_original):
        df = pd.read_csv(file_original)
        print(f"Dados carregados: {df.shape[0]} linhas, {df.shape[1]} colunas.\n")
    else:
        print(f"Erro: O arquivo {file_original} n√£o foi encontrado.")
        exit()


    # =============================================================================
    # 3. Relat√≥rio de dados
    # =============================================================================

    # Salvar a sa√≠da padr√£o original para restaurar posteriormente
    original_stdout = sys.stdout

    # Caminho do arquivo onde a sa√≠da ser√° salva, usando o caminho do s√≠mbolo e timeframe
    log_file_path = os.path.join(symbol_timeframe_path, f"{symbol}_{timeframe}_relatorio.doc")

    # Abre o arquivo .doc para grava√ß√£o. Se o arquivo n√£o existir, ser√° criado.
    log_file = open(log_file_path, 'w', encoding='utf-8')

    # Redireciona a sa√≠da padr√£o para o arquivo de log, para que tudo o que for impresso seja gravado no arquivo
    sys.stdout = log_file

    # Impress√µes no arquivo de relat√≥rio com informa√ß√µes sobre o ativo e timeframe
    print(f"RELAT√ìRIO CRIADO:  {symbol}\n")
    print(f"TIMEFRAME:  {timeframe}\n")


    # =============================================================================
    # 4. Converter 'time' para datetime
    # =============================================================================

    # Converte a coluna 'time' do DataFrame de timestamps (em formato Unix) para o formato datetime
    df['time'] = pd.to_datetime(df['time'])

    # Exibe uma mensagem indicando que a convers√£o foi conclu√≠da com sucesso
    print("Convers√£o da coluna 'time' para datetime conclu√≠da!\n")


    # =============================================================================
    # 5. Gerar o R√≥tulo (Target) para Revers√£o de Tend√™ncia
    # =============================================================================

    # Fun√ß√£o para verificar a exist√™ncia de uma coluna no DataFrame
    def check_column_exists(df, column_name):
        # Verifica se a coluna especificada existe no DataFrame
        if column_name not in df.columns:
            # Exibe uma mensagem de erro caso a coluna n√£o seja encontrada
            print(f"Erro: A coluna '{column_name}' n√£o foi encontrada no DataFrame.")
            return False  # Retorna False caso a coluna n√£o exista
        return True  # Retorna True caso a coluna exista

    # Verifica√ß√£o antes de acessar a coluna 'target'
    if not check_column_exists(df, 'target'):
        # Caso a coluna 'target' n√£o exista, ela ser√° criada
        print("Criando a coluna 'target' para revers√£o de tend√™ncia...")

        # Calcula o retorno futuro, deslocando os pre√ßos de fechamento para o futuro (N per√≠odos)
        df['future_return'] = df['close'].shift(-N) / df['close'] - 1

        # Calcula o retorno atual (percentual de mudan√ßa)
        df['current_return'] = df['close'].pct_change()

        # Cria a coluna 'target' com valores iniciais de 0 (neutro)
        df['target'] = 0

        # Define o r√≥tulo como 1 para revers√£o de tend√™ncia para cima (compra)
        # quando o retorno atual √© negativo e o retorno futuro √© maior que o threshold
        df.loc[(df['current_return'] < 0) & (df['future_return'] > threshold), 'target'] = 1

        # Define o r√≥tulo como -1 para revers√£o de tend√™ncia para baixo (venda)
        # quando o retorno atual √© positivo e o retorno futuro √© menor que o threshold negativo
        df.loc[(df['current_return'] > 0) & (df['future_return'] < -threshold), 'target'] = -1

        # Remove as √∫ltimas N linhas para evitar dados incompletos ao usar o deslocamento (shift)
        df = df.iloc[:-N]

        # Reseta o √≠ndice do DataFrame para manter a consist√™ncia ap√≥s o corte das linhas
        df.reset_index(drop=True, inplace=True)

        # Exibe uma mensagem informando que a coluna 'target' foi criada
        print("Coluna 'target' criada.\n")

    # Agora a coluna 'target' pode ser acessada com seguran√ßa
    y = df['target']


    # =============================================================================
    # 6. Definir as Features para o Modelo
    # =============================================================================

    # Lista com os nomes de todas as poss√≠veis features que podem ser usadas no modelo
    features = [
        '+DM', '+DM_sum', '-DM', '-DM_sum', 'adl', 'adf_stat_30', 'ask', 'atr_14', 'atr_50', 'avg_price', 'bear_power',
        'beta', 'bid', 'bollinger_percent_b', 'bull_power', 'cci_14', 'change_percent', 'cmf_20', 'close',
        'close_lag1', 'close_lag2', 'close_lag3', 'cum_max', 'drawdown', 'ema_13', 'ema_20', 'ewma_30', 'high', 'high_lag1',
        'high_lag2', 'high_lag3', 'hma_14', 'hma_50', 'last_price', 'low', 'low_lag1', 'low_lag2', 'low_lag3', 'log_ret', 'macd',
        'macd_hist', 'macd_signal', 'momentum_10', 'mfi_10', 'open', 'open_lag1', 'open_lag2', 'open_lag3', 'obv', 'roc_20',
        'roc_5', 'rsi_14', 'sharpe_ratio', 'sma_100', 'sma_200', 'sma_21', 'sma_50', 'sma_9', 'sma_cross_signal_50_200',
        'sma_cross_signal_9_21', 'sma_diff_50_200', 'sma_diff_9_21', 'spread', 'stoch_d', 'stoch_k', 'tick_volume',
        'TR_sum', 'ulcer_index', 'vwap_20', 'volume_profile', 'vw_rsi', 'vroc_14', 'williams_r_14', 'zscore_vol'
    ]

    # Filtra as features dispon√≠veis no DataFrame, garantindo que as features que existem no DataFrame sejam selecionadas
    available_features = [feat for feat in features if feat in df.columns]

    # Seleciona as colunas do DataFrame que correspondem √†s features dispon√≠veis
    X = df[available_features]

    # Define a coluna 'target' como a vari√°vel dependente (r√≥tulo) para o modelo
    y = df['target']

    # Exibe as features definidas que foram encontradas no DataFrame
    print(f"Features definidas: {available_features}\n")


    # =============================================================================
    # 7. Imputa√ß√£o dos Valores Ausentes e Normaliza√ß√£o
    # =============================================================================

    # Cria um objeto SimpleImputer para imputar valores ausentes utilizando a m√©dia
    imputer = SimpleImputer(strategy='mean')

    # Aplica a imputa√ß√£o dos valores ausentes no conjunto de features X
    # O m√©todo fit_transform ajusta o imputer aos dados e aplica a transforma√ß√£o, substituindo valores ausentes pela m√©dia
    X_imputed = imputer.fit_transform(X)

    # Cria um objeto StandardScaler para normalizar os dados
    scaler = StandardScaler()

    # Aplica a normaliza√ß√£o (padroniza√ß√£o) nos dados imputados, transformando para ter m√©dia 0 e desvio padr√£o 1
    X_scaled = scaler.fit_transform(X_imputed)

    # Exibe uma mensagem informando que a imputa√ß√£o e a normaliza√ß√£o foram conclu√≠das com sucesso
    print("Imputa√ß√£o e normaliza√ß√£o conclu√≠das!\n")


    # =============================================================================
    # 8. Dividir em Treino e Teste
    # =============================================================================

    # Utiliza a fun√ß√£o train_test_split do scikit-learn para dividir os dados em conjuntos de treino e teste
    # X_scaled: As features normalizadas
    # y: O alvo (target) a ser previsto
    # test_size=0.7: Define que 70% dos dados ser√£o usados para o conjunto de teste e 30% para treino
    # random_state=42: Garante que a divis√£o dos dados seja reprodut√≠vel, ou seja, os mesmos dados de treino e teste ser√£o gerados em cada execu√ß√£o
    # stratify=y: Garante que a divis√£o entre treino e teste preserve a distribui√ß√£o da vari√°vel alvo (y) nos dois conjuntos
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.7, random_state=42, stratify=y
    )

    # Exibe uma mensagem informando que os dados foram divididos corretamente em treino e teste
    print("Dados divididos em treino e teste!\n")


    # =============================================================================
    # 9. Modelo RandomForest e Avalia√ß√£o
    # =============================================================================

    # Exibe uma mensagem informando que o modelo RandomForest est√° sendo treinado
    print("Treinando modelo RandomForest...")

    # Cria√ß√£o do modelo RandomForestClassifier com par√¢metros espec√≠ficos
    # n_estimators=100: Define o n√∫mero de √°rvores na floresta. Quanto maior esse n√∫mero, mais preciso o modelo pode ser, mas tamb√©m mais computacionalmente caro.
    # max_depth=None: N√£o limita a profundidade das √°rvores. Isso permite que as √°rvores cres√ßam sem restri√ß√µes, o que pode aumentar a complexidade do modelo.
    # random_state=42: Garante que o modelo seja reprodut√≠vel, ou seja, os mesmos resultados ser√£o gerados em cada execu√ß√£o
    # n_jobs=-1: Utiliza todos os n√∫cleos do processador para treinar o modelo, acelerando o processo
    model_best = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42, n_jobs=-1)

    # Treina o modelo com os dados de treino (X_train e y_train)
    model_best.fit(X_train, y_train)

    # Faz a previs√£o com os dados de teste
    y_pred_test = model_best.predict(X_test)

    # Avalia o modelo utilizando a acur√°cia, comparando as previs√µes (y_pred_test) com os valores reais (y_test)
    accuracy = accuracy_score(y_test, y_pred_test)

    # Exibe a acur√°cia do modelo, mostrando o desempenho do modelo no conjunto de dados de teste
    print(f"\nAcur√°cia do Modelo: {accuracy:.2%}\n")

    # =============================================================================
    # 10. An√°lise de Import√¢ncia das Features
    # =============================================================================

    # Obt√©m a import√¢ncia das features do modelo RandomForest treinado
    # O atributo 'feature_importances_' do modelo RandomForest retorna a import√¢ncia de cada feature
    importances = model_best.feature_importances_

    # Cria um objeto pd.Series a partir das import√¢ncias das features e ordena em ordem decrescente
    # O √≠ndice do pd.Series √© as features dispon√≠veis que foram usadas no modelo
    feat_importance = pd.Series(importances, index=available_features).sort_values(ascending=False)

    # Gera um gr√°fico de barras para visualizar a import√¢ncia das features
    # 'kind='bar'' especifica que o gr√°fico ser√° do tipo barra, e 'figsize=(10, 6)' define o tamanho da figura
    feat_importance.plot(kind='bar', title='Import√¢ncia das Features', figsize=(10, 6))

    # Define o caminho para salvar o gr√°fico de import√¢ncia das features em formato PNG
    # O caminho √© baseado no s√≠mbolo e timeframe, e o nome do arquivo inclui o sufixo '_feature_importance_sorted_by_value'
    plot_filename = os.path.join(symbol_timeframe_path, f"{symbol}_{timeframe}_feature_importance_RandomForest_treinado.png")

    # Salva o gr√°fico gerado no arquivo especificado
    plt.savefig(plot_filename)

    # Exibe uma mensagem informando o local onde o gr√°fico foi salvo
    print(f"Gr√°fico de Import√¢ncia das Features (ordenado por valor) salvo em: {plot_filename}")

    # Fecha o gr√°fico ap√≥s salvar para liberar mem√≥ria
    plt.close()


    # =============================================================================
    # 11 Fun√ß√£o Testar Melhor Thresholds
    # =============================================================================



        # Converte os resultados para um DataFrame
        df_resultados = pd.DataFrame(resultados, columns=["Threshold", "N_Features", "Accuracy"])

        # Exibe os resultados no console
        print("\nüîç Compara√ß√£o de Acur√°cia por Threshold:")
        print(df_resultados.to_string(index=False))

        # Gera o gr√°fico de compara√ß√£o entre thresholds e acur√°cia
        plt.figure(figsize=(10, 6))
        plt.plot(df_resultados["Threshold"], df_resultados["Accuracy"], marker='o')
        plt.title("Varia√ß√£o da Acur√°cia com Thresholds de Import√¢ncia")
        plt.xlabel("Threshold de Import√¢ncia")
        plt.ylabel("Acur√°cia")
        plt.grid(True)

        # Define o caminho para salvar o gr√°fico
        plot_filename = os.path.join(symbol_timeframe_path, f"{symbol}_{timeframe}_threshold_vs_accuracy.png")
        plt.savefig(plot_filename)

        # Exibe o caminho onde o gr√°fico foi salvo
        print(f"Gr√°fico de an√°lise de thresholds salvo em: {plot_filename}")
        plt.close()

        # Retorna o DataFrame com os resultados
        return df_resultados


    # =============================================================================
    # 12. An√°lise de Permutation Importance das Features e Sele√ß√£o das Relevantes
    # =============================================================================

    # Criar um DataFrame com as features nomeadas para an√°lise, a partir dos dados normalizados
    X_df = pd.DataFrame(X_scaled, columns=available_features)

    # Rodar a an√°lise comparativa de thresholds (definido anteriormente)
    # Isso testa o impacto de diferentes thresholds de import√¢ncia na acur√°cia do modelo
    testar_thresholds_importancia(model_best, X_df, y, symbol_timeframe_path, symbol, timeframe)

    # Continuar com a an√°lise de Permutation Importance para determinar as features mais relevantes
    # Exibe uma mensagem indicando o in√≠cio da an√°lise de Permutation Importance
    print("Analisando Permutation Importance das Features (para sele√ß√£o final)...")

    # Calcula a Permutation Importance no conjunto de teste usando o modelo RandomForest
    # n_repeats=10 faz 10 permuta√ß√µes para calcular a import√¢ncia m√©dia de cada feature
    # random_state=42 garante que os resultados sejam reproduz√≠veis
    perm_result = permutation_importance(model_best, X_test, y_test, n_repeats=10, random_state=42)

    # Atribui a import√¢ncia das features calculada pela permuta√ß√£o em um pd.Series
    # index=available_features assegura que cada valor de import√¢ncia seja associado √† sua respectiva feature
    perm_importance = pd.Series(perm_result.importances_mean, index=available_features).sort_values(ascending=False)

    # Exibe a Permutation Importance das features no console
    # As features com maior import√¢ncia ser√£o exibidas no topo
    print("\nüîç Permutation Importance das Features:")
    print(perm_importance)

    # Gera um gr√°fico de barras para visualizar a Permutation Importance das features
    # 'kind='bar'' especifica que o gr√°fico ser√° do tipo barra, e 'figsize=(10, 6)' define o tamanho do gr√°fico
    perm_importance.plot(kind='bar', title='Permutation Importance das Features', figsize=(10, 6))

    # Define o caminho para salvar o gr√°fico gerado
    plot_filename_perm = os.path.join(symbol_timeframe_path, f"{symbol}_{timeframe}_permutation_importance_mean_sorted.png")

    # Salva o gr√°fico de Permutation Importance como uma imagem PNG
    plt.savefig(plot_filename_perm)

    # Exibe uma mensagem informando onde o gr√°fico foi salvo
    print(f"Gr√°fico de Permutation Importance das Features (m√©dia ordenada) salvo em: {plot_filename_perm}")
    plt.close()  # Fecha o gr√°fico para liberar mem√≥ria

    # Seleciona as features cuja import√¢ncia √© maior que o limiar (threshold)
    selected_features = perm_importance[perm_importance > importance_threshold].index.tolist()

    # Seleciona os dados de entrada (features) com base nas features selecionadas
    X_selected = df[selected_features]

    # Atualiza o dataset com as features selecionadas, imputando valores ausentes e normalizando os dados
    imputer = SimpleImputer(strategy='mean')  # Imputa valores ausentes usando a m√©dia
    X_imputed = imputer.fit_transform(X_selected)  # Aplica a imputa√ß√£o
    scaler = StandardScaler()  # Cria o normalizador
    X_scaled = scaler.fit_transform(X_imputed)  # Normaliza os dados

    # Divide os dados em conjuntos de treino e teste
    # test_size=0.3 define que 30% dos dados ir√£o para o conjunto de teste
    # random_state=42 garante que a divis√£o seja reprodut√≠vel
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.3, random_state=42, stratify=y
    )

    # Salva as features selecionadas para uso futuro, armazenando-as em um arquivo pickle
    selected_features_path = os.path.join(symbol_timeframe_path, f'{symbol}_{timeframe}_selected_features.pkl')
    joblib.dump(selected_features, selected_features_path)

    # Exibe uma mensagem indicando que as features selecionadas foram salvas com sucesso
    print(f"Features selecionadas salvas em '{selected_features_path}'.")

    # Re-treina o modelo RandomForest usando as features selecionadas
    print("\nTreinando modelo RandomForest com features selecionadas...")
    model_best = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42, n_jobs=-1)
    model_best.fit(X_train, y_train)  # Treina o modelo com os dados de treino

    # Avalia o modelo no conjunto de teste e calcula a acur√°cia
    accuracy = accuracy_score(y_test, model_best.predict(X_test))

    # Exibe a nova acur√°cia do modelo ap√≥s a sele√ß√£o das features
    print(f"\nüîç Nova Acur√°cia do Modelo com Features Selecionadas: {accuracy:.2%}\n")


    # =============================================================================
    # 13 Ajuste Fino dos Hiperpar√¢metros com GridSearchCV
    # =============================================================================

    # Exibe uma mensagem informando o in√≠cio da busca pelos melhores hiperpar√¢metros
    # A busca √© feita utilizando a t√©cnica de ajuste fino de par√¢metros com o GridSearchCV ou RandomizedSearchCV
    print("Iniciando busca pelos melhores hiperpar√¢metros com GridSearchCV...")

    # Definindo a grade de par√¢metros a ser explorada
    # A grade cont√©m uma lista de valores para os hiperpar√¢metros do modelo RandomForest que ser√£o testados
    # O n√∫mero de combina√ß√µes √© reduzido para evitar a explora√ß√£o excessiva do espa√ßo de par√¢metros

    tuned_param_grid = {
        'n_estimators': [100, 200, 300],  # N√∫mero de √°rvores na floresta
        'max_depth': [None, 10, 15, 20],  # Profundidade m√°xima das √°rvores
        'min_samples_split': [2, 5, 10],  # N√∫mero m√≠nimo de amostras necess√°rias para dividir um n√≥
        'min_samples_leaf': [1, 2],  # N√∫mero m√≠nimo de amostras necess√°rias para ser uma folha
        'max_features': ['sqrt', 'log2'],  # N√∫mero m√°ximo de features a serem consideradas para dividir um n√≥
        'bootstrap': [True]  # Usar amostragem bootstrap para treinar as √°rvores
    }

    """
    Coment√°rio sobre os melhores par√¢metros encontrados:
    Os melhores par√¢metros encontrados com base em uma busca anterior poderiam ser:
    {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 1, 
     'max_features': 'sqrt', 'max_depth': None, 'bootstrap': True}
    """

    # Instancia o modelo RandomForestClassifier, que ser√° ajustado com os melhores par√¢metros
    # O par√¢metro random_state=42 garante que os resultados sejam reprodut√≠veis
    model = RandomForestClassifier(random_state=42, n_jobs=-1)

    # RandomizedSearchCV permite explorar aleatoriamente um n√∫mero espec√≠fico de combina√ß√µes de par√¢metros
    # n_iter=50 define o n√∫mero de combina√ß√µes aleat√≥rias de par√¢metros que o RandomizedSearchCV testar√°
    # cv=3 indica que ser√° usado 3-fold cross-validation para avaliar cada combina√ß√£o
    # verbose=2 mostra detalhes sobre o progresso da busca
    # n_jobs=-1 utiliza todos os n√∫cleos do processador para acelerar a busca
    random_search = RandomizedSearchCV(
        estimator=model,  # O modelo a ser ajustado
        param_distributions=tuned_param_grid,  # A grade de par√¢metros a ser testada
        n_iter=50,  # N√∫mero de combina√ß√µes aleat√≥rias a serem testadas
        cv=3,  # N√∫mero de folds para cross-validation
        verbose=2,  # N√≠vel de detalhes sobre o progresso da busca
        random_state=42,  # Garante a reprodutibilidade da busca
        n_jobs=-1  # Utiliza todos os n√∫cleos do processador
    )

    # Ajusta o modelo aos dados de treino, buscando os melhores par√¢metros
    random_search.fit(X_train, y_train)

    # Exibe os melhores par√¢metros encontrados durante a busca
    # O m√©todo 'best_params_' retorna o dicion√°rio com os melhores par√¢metros encontrados
    print("\nMelhores par√¢metros encontrados:")
    print(random_search.best_params_)

    # Atualiza o modelo com os melhores par√¢metros encontrados
    # O modelo com os melhores par√¢metros √© acessado atrav√©s do atributo 'best_estimator_'
    model_best = random_search.best_estimator_

    # Avalia√ß√£o com os melhores hiperpar√¢metros encontrados
    # Utiliza o modelo ajustado com os melhores par√¢metros para fazer previs√µes no conjunto de teste
    y_pred_test = model_best.predict(X_test)

    # Calcula a acur√°cia do modelo nas previs√µes feitas com os dados de teste
    accuracy = accuracy_score(y_test, y_pred_test)

    # Exibe a acur√°cia do modelo utilizando os melhores hiperpar√¢metros
    print(f"\n\U0001F50D Acur√°cia do Modelo (Teste) com melhores hiperpar√¢metros: {accuracy:.2%}\n")


    # =============================================================================
    # 14. Salvar o Melhor Modelo, Imputer e Scaler
    # =============================================================================

    # O objetivo desta etapa √© salvar o modelo treinado, o imputer (para lidar com valores ausentes)
    # e o scaler (para normaliza√ß√£o dos dados) em arquivos para uso futuro. Esses arquivos s√£o
    # salvos no diret√≥rio espec√≠fico para o s√≠mbolo e o timeframe, para facilitar a reutiliza√ß√£o
    # do modelo e das transforma√ß√µes em futuras previs√µes.

    # Salva o modelo treinado (modelo com os melhores par√¢metros encontrados) em um arquivo .pkl
    # O arquivo ser√° armazenado na pasta referente ao s√≠mbolo e timeframe, com o nome 'best_model_grid_search.pkl'
    joblib.dump(model_best, os.path.join(symbol_timeframe_path, f'{symbol}_{timeframe}_best_model_grid_search.pkl'))

    # Exibe uma mensagem indicando onde o modelo foi salvo
    print(f"Modelo salvo em '{symbol_timeframe_path}/{symbol}_{timeframe}_best_model_grid_search.pkl'.")

    # Salva o imputer (usado para preencher valores ausentes com a m√©dia, no caso) em um arquivo .pkl
    # O arquivo ser√° armazenado na mesma pasta, com o nome 'imputer.pkl'
    joblib.dump(imputer, os.path.join(symbol_timeframe_path, f'{symbol}_{timeframe}_imputer.pkl'))

    # Exibe uma mensagem indicando onde o imputer foi salvo
    print(f"Imputer salvo em '{symbol_timeframe_path}/{symbol}_{timeframe}_imputer.pkl'.")

    # Salva o scaler (usado para normalizar os dados) em um arquivo .pkl
    # O arquivo ser√° armazenado na mesma pasta, com o nome 'scaler.pkl'
    joblib.dump(scaler, os.path.join(symbol_timeframe_path, f'{symbol}_{timeframe}_scaler.pkl'))

    # Exibe uma mensagem indicando onde o scaler foi salvo
    print(f"Scaler salvo em '{symbol_timeframe_path}/{symbol}_{timeframe}_scaler.pkl'.\n")


    # =============================================================================
    # 15. An√°lise de Import√¢ncia das Features
    # =============================================================================

    # Inicia a an√°lise de import√¢ncia das features utilizando o modelo RandomForest
    print("Analisando import√¢ncia das features...")

    # Acessa a import√¢ncia das features do modelo RandomForest
    # O atributo 'feature_importances_' retorna a import√¢ncia de cada feature para o modelo
    importances = model_best.feature_importances_

    # Cria um pd.Series com as import√¢ncias, associando a cada feature sua respectiva import√¢ncia
    # O resultado √© ordenado em ordem decrescente para facilitar a visualiza√ß√£o das features mais importantes
    feat_importance = pd.Series(importances, index=selected_features).sort_values(ascending=False)

    # Exibe as import√¢ncias das features no console
    print("\nüîç Import√¢ncia das Features (RandomForest):")
    print(feat_importance)

    # Gera um gr√°fico de barras para visualizar a import√¢ncia das features
    # 'kind='bar'' especifica que o gr√°fico ser√° do tipo barra
    # 'figsize=(10, 6)' define o tamanho do gr√°fico
    feat_importance.plot(kind='bar', title='Import√¢ncia das Features', figsize=(10, 6))

    # Define o caminho para salvar o gr√°fico na pasta "Train", organizando os gr√°ficos por s√≠mbolo e timeframe
    symbol_timeframe_path = os.path.join(base_path, f"{symbol}_{timeframe}")  # Caminho para o diret√≥rio do s√≠mbolo e timeframe

    # Verifica se a pasta para salvar o gr√°fico existe, se n√£o, cria a pasta
    if not os.path.exists(symbol_timeframe_path):
        os.makedirs(symbol_timeframe_path)

    # Define o nome do arquivo onde o gr√°fico ser√° salvo
    plot_filename = os.path.join(symbol_timeframe_path, f"{symbol}_{timeframe}_feature_importance2.png")

    # Salva o gr√°fico de import√¢ncia das features como uma imagem PNG
    plt.savefig(plot_filename)
    print(f"Gr√°fico de Import√¢ncia das Features salvo em: {plot_filename}")

    # Fecha o gr√°fico para liberar os recursos de mem√≥ria utilizados na cria√ß√£o do gr√°fico
    plt.close()


    # =============================================================================
    # 16. An√°lise de Permutation Importance das Features
    # =============================================================================
    # Realiza a an√°lise de Permutation Importance das features
    # A permutation importance verifica o impacto da permuta√ß√£o de uma feature na performance do modelo
    perm_result = permutation_importance(model_best, X_test, y_test, n_repeats=10, random_state=42)

    # Cria um pd.Series com a m√©dia da import√¢ncia das features, ordenando por import√¢ncia em ordem decrescente
    perm_importance = pd.Series(perm_result.importances_mean, index=selected_features).sort_values(ascending=False)

    # Exibe a Permutation Importance das features no console
    print("\nüîç Permutation Importance das Features:")
    print(perm_importance)

    # Gera um gr√°fico de barras para visualizar a Permutation Importance das features
    perm_importance.plot(kind='bar', title='Permutation Importance das Features', figsize=(10, 6))

    # Define o caminho para salvar o gr√°fico da Permutation Importance
    # O gr√°fico ser√° salvo na mesma pasta "Train", com o nome do arquivo espec√≠fico para Permutation Importance
    plot_filename_perm = os.path.join(symbol_timeframe_path, f"{symbol}_{timeframe}_permutation_importance3.png")

    # Salva o gr√°fico de Permutation Importance das features como uma imagem PNG
    plt.savefig(plot_filename_perm)
    print(f"Gr√°fico de Permutation Importance das Features salvo em: {plot_filename_perm}")

    # Fecha o gr√°fico para liberar os recursos de mem√≥ria utilizados na cria√ß√£o do gr√°fico
    plt.close()


    # =============================================================================
    # 17. Aplicar o Modelo em Todo o Conjunto para o Backtest
    # =============================================================================
    # Aplica o modelo treinado (model_best) no conjunto completo de dados (X_scaled)
    # A vari√°vel X_scaled cont√©m as features de entrada, que j√° foram normalizadas e preparadas
    # O modelo realiza previs√µes para cada linha do conjunto de dados
    print("Aplicando o modelo no conjunto completo para backtest...")

    # A coluna 'Predi√ß√£o' √© adicionada ao DataFrame 'df', com as previs√µes do modelo
    # Cada linha do DataFrame recebe um valor de previs√£o (-1, 0, ou 1) com base no modelo treinado
    df['Predi√ß√£o'] = model_best.predict(X_scaled)

    # Mensagem indicando que a aplica√ß√£o do modelo foi conclu√≠da
    print("Modelo aplicado.\n")


    # =============================================================================
    # 18. Preparar o DataFrame para o Backtest (todas as linhas)
    # =============================================================================
    # Exibe uma mensagem indicando que a prepara√ß√£o do DataFrame para o backtest est√° em andamento
    print("Preparando DataFrame para o backtest...")

    # Inicializa as colunas do DataFrame com valores padr√£o
    # "Estado" indica o status da opera√ß√£o (compra, venda, sem opera√ß√£o)
    df["Estado"] = "Sem Opera√ß√£o"  # Inicialmente, n√£o h√° opera√ß√£o aberta
    df["Entrada"] = np.nan  # Pre√ßo de entrada ser√° definido mais tarde
    df["Sa√≠da"] = np.nan  # Pre√ßo de sa√≠da ser√° definido durante o fechamento da opera√ß√£o
    df["Lucro/Preju√≠zo"] = 0.0  # Lucro ou preju√≠zo de cada opera√ß√£o ser√° calculado durante o backtest
    df["Resultado (%)"] = 0.0  # Resultado percentual de cada opera√ß√£o
    df["Acumulado"] = np.nan  # Resultado acumulado de cada opera√ß√£o

    # Confirma a prepara√ß√£o do DataFrame
    print("DataFrame preparado.\n")

    # =============================================================================
    # Simula√ß√£o Sequencial dos Trades com Fechamento Autom√°tico √†s 16:45:00
    # =============================================================================
    # Exibe a mensagem indicando que a simula√ß√£o do backtest est√° come√ßando
    print("Iniciando simula√ß√£o do backtest...")

    # Vari√°veis para controlar o estado da opera√ß√£o de trade
    trade_aberto = False  # Inicia com nenhuma opera√ß√£o aberta
    trade_direcao = None  # Pode ser "Long" ou "Short", define a dire√ß√£o do trade
    preco_entrada = None  # O pre√ßo de entrada ser√° atribu√≠do quando uma opera√ß√£o for aberta
    num_acoes = None  # N√∫mero de a√ß√µes compradas ou vendidas
    trade_start_index = None  # O √≠ndice onde o trade come√ßou no DataFrame

    # Loop que percorre todas as linhas do DataFrame para simular os trades
    for i in range(len(df)):
        # Se n√£o houver opera√ß√£o aberta, verifica o sinal da previs√£o (coluna 'Predi√ß√£o')
        if not trade_aberto:
            sinal = df.loc[i, "Predi√ß√£o"]  # Verifica o sinal gerado pelo modelo
            if sinal == 1:
                # Sinal de compra ("Long"), define as vari√°veis e marca o in√≠cio da opera√ß√£o
                trade_direcao = "Long"
                preco_entrada = df.loc[i, "close"] + spread  # Pre√ßo de entrada com o spread
                num_acoes = (valor_por_trade * alavancagem) / preco_entrada  # C√°lculo de quantas a√ß√µes comprar
                trade_start_index = i  # Salva o √≠ndice de in√≠cio do trade
                df.loc[i, "Estado"] = "Compra"  # Marca a opera√ß√£o como "Compra"
                df.loc[i, "Entrada"] = preco_entrada  # Registra o pre√ßo de entrada
                df.loc[i, "Acumulado"] = 0.0  # Inicializa o resultado acumulado
                trade_aberto = True  # Indica que a opera√ß√£o foi aberta
            elif sinal == -1:
                # Sinal de venda ("Short"), define as vari√°veis e marca o in√≠cio da opera√ß√£o
                trade_direcao = "Short"
                preco_entrada = df.loc[i, "close"] - spread  # Pre√ßo de entrada com o spread
                num_acoes = (valor_por_trade * alavancagem) / preco_entrada  # C√°lculo de quantas a√ß√µes vender
                trade_start_index = i  # Salva o √≠ndice de in√≠cio do trade
                df.loc[i, "Estado"] = "Venda"  # Marca a opera√ß√£o como "Venda"
                df.loc[i, "Entrada"] = preco_entrada  # Registra o pre√ßo de entrada
                df.loc[i, "Acumulado"] = 0.0  # Inicializa o resultado acumulado
                trade_aberto = True  # Indica que a opera√ß√£o foi aberta
            else:
                # Caso n√£o haja opera√ß√£o (sinal == 0), marca como "Sem Opera√ß√£o"
                df.loc[i, "Estado"] = "Sem Opera√ß√£o"
        else:
            # Se houver opera√ß√£o aberta, verifica o sinal da previs√£o para encerrar a opera√ß√£o
            sinal = df.loc[i, "Predi√ß√£o"]
            if trade_direcao == "Long" and sinal == -1:
                # Se for uma posi√ß√£o Long e o sinal for de venda, fecha a opera√ß√£o
            preco_saida = df.loc[i, "close"] - spread  # Pre√ßo de sa√≠da com o spread
            resultado = (preco_saida - spread) - preco_entrada  # C√°lculo do resultado
            df.loc[i, "Estado"] = "Revers√£o Long"  # Marca como revers√£o de longo
            df.loc[i, "Sa√≠da"] = preco_saida  # Registra o pre√ßo de sa√≠da
            df.loc[i, "Lucro/Preju√≠zo"] = num_acoes * resultado  # Calcula lucro/preju√≠zo
            df.loc[i, "Acumulado"] = num_acoes * resultado  # Resultado acumulado
            trade_aberto = False  # Fecha o trade
            trade_direcao = None  # Zera a dire√ß√£o do trade
            preco_entrada = None  # Zera o pre√ßo de entrada
            num_acoes = None  # Zera o n√∫mero de a√ß√µes
            trade_start_index = None  # Zera o √≠ndice de in√≠cio
        elif trade_direcao == "Short" and sinal == 1:
            # Se for uma posi√ß√£o Short e o sinal for de compra, fecha a opera√ß√£o
            preco_saida = df.loc[i, "close"] + spread  # Pre√ßo de sa√≠da com o spread
            resultado = preco_entrada - (preco_saida + spread)  # C√°lculo do resultado
            df.loc[i, "Estado"] = "Revers√£o Short"  # Marca como revers√£o de curto
            df.loc[i, "Sa√≠da"] = preco_saida  # Registra o pre√ßo de sa√≠da
            df.loc[i, "Lucro/Preju√≠zo"] = num_acoes * resultado  # Calcula lucro/preju√≠zo
            df.loc[i, "Acumulado"] = num_acoes * resultado  # Resultado acumulado
            trade_aberto = False  # Fecha o trade
            trade_direcao = None  # Zera a dire√ß√£o do trade
            preco_entrada = None  # Zera o pre√ßo de entrada
            num_acoes = None  # Zera o n√∫mero de a√ß√µes
            trade_start_index = None  # Zera o √≠ndice de in√≠cio
        else:
            # Caso o sinal n√£o seja de revers√£o, mant√©m o trade aberto e calcula o valor acumulado
            if trade_direcao == "Long":
                acumulado = (df.loc[i, "close"] - spread - preco_entrada) * num_acoes  # C√°lculo do acumulado para Long
                df.loc[i, "Estado"] = "Comprado"  # Marca a opera√ß√£o como "Comprado"
            elif trade_direcao == "Short":
                acumulado = (preco_entrada - (df.loc[i, "close"] + spread)) * num_acoes  # C√°lculo do acumulado para Short
                df.loc[i, "Estado"] = "Vendido"  # Marca a opera√ß√£o como "Vendido"
            df.loc[i, "Acumulado"] = acumulado  # Atualiza o valor acumulado

    # Finaliza o backtest e exibe uma mensagem
    print("Simula√ß√£o do backtest conclu√≠da.\n")


    # =============================================================================
    # 19. An√°lise dos Resultados dos √öltimos 30 Dias
    # =============================================================================
    # Exibe a mensagem indicando que a an√°lise dos resultados dos √∫ltimos 30 dias est√° come√ßando
    print("Analisando resultados dos √∫ltimos 30 dias...")

    # Adiciona uma nova coluna 'date' no DataFrame, extraindo apenas a data (sem o hor√°rio) da coluna 'time'
    df['date'] = df['time'].dt.date

    # Agrupa os resultados di√°rios, somando o lucro/preju√≠zo, contando o n√∫mero de transa√ß√µes,
    # o n√∫mero de transa√ß√µes com lucro e o n√∫mero de transa√ß√µes com preju√≠zo
    daily_results = df.groupby('date').agg(
        total_profit_loss=('Lucro/Preju√≠zo', 'sum'),  # Soma do lucro/preju√≠zo di√°rio
        num_transactions=('Lucro/Preju√≠zo', 'size'),  # Conta o n√∫mero total de transa√ß√µes por dia
        num_profit_transactions=('Lucro/Preju√≠zo', lambda x: (x > 0).sum()),  # Conta transa√ß√µes com lucro
        num_loss_transactions=('Lucro/Preju√≠zo', lambda x: (x < 0).sum())  # Conta transa√ß√µes com preju√≠zo
    )

    # Encontra a data mais recente nos resultados di√°rios
    max_date = daily_results.index.max()

    # Filtra os resultados dos √∫ltimos 30 dias, baseado na data m√°xima encontrada
    last_30_days = daily_results[daily_results.index >= (max_date - pd.Timedelta(days=30))]

    # Reseta o √≠ndice para obter um DataFrame com um √≠ndice num√©rico sequencial
    last_30_days_df = last_30_days.reset_index()

    # Exibe os resultados dos √∫ltimos 30 dias
    print("\nüîç An√°lise dos Resultados dos √öltimos 30 Dias:")
    print(last_30_days_df.to_string(index=False))

    # Calcula o total de lucro/preju√≠zo nos √∫ltimos 30 dias
    total_30 = last_30_days['total_profit_loss'].sum()

    # Calcula a m√©dia di√°ria de lucro/preju√≠zo nos √∫ltimos 30 dias
    mean_30 = last_30_days['total_profit_loss'].mean()

    # Calcula o desvio padr√£o dos resultados di√°rios
    std_30 = daily_results['total_profit_loss'].std()

    # Exibe o total, m√©dia di√°ria e desvio padr√£o dos resultados dos √∫ltimos 30 dias
    print(f"\nTotal (30 dias): R$ {total_30:.2f}")
    print(f"M√©dia Di√°ria: R$ {mean_30:.2f}")
    print(f"Desvio Padr√£o: R$ {std_30:.2f}\n")


    # =============================================================================
    # 20. C√°lculo da M√©dia Mensal de Ganho
    # =============================================================================
    # Exibe a mensagem indicando que o c√°lculo da m√©dia mensal de ganho est√° come√ßando
    print("Calculando m√©dia mensal de ganho...")

    # Cria uma nova coluna 'month' no DataFrame, convertendo a coluna 'time' para o per√≠odo mensal
    # Isso ajuda a agrupar os dados por m√™s, ignorando o dia e o ano
    df['month'] = df['time'].dt.to_period('M')

    # Agrupa os dados por m√™s, calculando o lucro/preju√≠zo total, o n√∫mero de transa√ß√µes, e o n√∫mero de transa√ß√µes lucrativas e prejudiciais
    monthly_results = df.groupby('month').agg(
        total_profit_loss=('Lucro/Preju√≠zo', 'sum'),  # Soma do lucro/preju√≠zo de cada m√™s
        num_transactions=('Lucro/Preju√≠zo', 'size'),  # Conta o n√∫mero total de transa√ß√µes de cada m√™s
        num_profit_transactions=('Lucro/Preju√≠zo', lambda x: (x > 0).sum()),  # Conta transa√ß√µes com lucro
        num_loss_transactions=('Lucro/Preju√≠zo', lambda x: (x < 0).sum())  # Conta transa√ß√µes com preju√≠zo
    )

    # Calcula a m√©dia do lucro/preju√≠zo total mensal
    monthly_mean = monthly_results['total_profit_loss'].mean()

    # Exibe os resultados mensais no console
    print("\nüîç M√©dia Mensal de Ganho:")
    # Exibe o DataFrame de resultados mensais, removendo o √≠ndice
    print(monthly_results.reset_index().to_string(index=False))

    # Exibe a m√©dia mensal de lucro/preju√≠zo
    print(f"\nM√©dia Mensal: R$ {monthly_mean:.2f}\n")


    # =============================================================================
    # 21. Estat√≠sticas das Opera√ß√µes Fechadas
    # =============================================================================
    # Exibe a mensagem indicando que o c√°lculo das estat√≠sticas das opera√ß√µes fechadas est√° come√ßando
    print("Calculando estat√≠sticas das opera√ß√µes fechadas...")

    # Filtra o DataFrame para incluir apenas as linhas onde a coluna 'Estado' indica que a opera√ß√£o foi fechada
    # Os estados "Fechado Hor√°rio", "Revers√£o Long" e "Revers√£o Short" indicam opera√ß√µes finalizadas
    fechadas = df[df["Estado"].isin(["Fechado Hor√°rio", "Revers√£o Long", "Revers√£o Short"])]

    # Calcula a quantidade de opera√ß√µes com lucro, contando quantos valores na coluna 'Lucro/Preju√≠zo' s√£o positivos
    qtd_profit = (fechadas["Lucro/Preju√≠zo"] > 0).sum()

    # Calcula a quantidade de opera√ß√µes com preju√≠zo, contando quantos valores na coluna 'Lucro/Preju√≠zo' s√£o negativos
    qtd_loss = (fechadas["Lucro/Preju√≠zo"] < 0).sum()

    # Calcula o somat√≥rio dos lucros, somando os valores da coluna 'Lucro/Preju√≠zo' que s√£o positivos
    soma_profit = fechadas.loc[fechadas["Lucro/Preju√≠zo"] > 0, "Lucro/Preju√≠zo"].sum()

    # Calcula o somat√≥rio dos preju√≠zos, somando os valores da coluna 'Lucro/Preju√≠zo' que s√£o negativos
    soma_loss = fechadas.loc[fechadas["Lucro/Preju√≠zo"] < 0, "Lucro/Preju√≠zo"].sum()

    # Calcula o maior lucro obtido em uma √∫nica opera√ß√£o
    max_gain = fechadas["Lucro/Preju√≠zo"].max()

    # Calcula o maior preju√≠zo obtido em uma √∫nica opera√ß√£o
    max_loss = fechadas["Lucro/Preju√≠zo"].min()

    # Exibe as estat√≠sticas calculadas no console
    print("\nüîç Estat√≠sticas das Opera√ß√µes Fechadas:")
    # Exibe a quantidade de opera√ß√µes com lucro
    print(f"Quantidade de opera√ß√µes com lucro: {qtd_profit}")
    # Exibe a quantidade de opera√ß√µes com preju√≠zo
    print(f"Quantidade de opera√ß√µes com preju√≠zo: {qtd_loss}")
    # Exibe o somat√≥rio dos lucros
    print(f"Somat√≥rio dos lucros: R$ {soma_profit:.2f}")
    # Exibe o somat√≥rio dos preju√≠zos
    print(f"Somat√≥rio dos preju√≠zos: R$ {soma_loss:.2f}")
    # Exibe o ganho m√°ximo em uma √∫nica opera√ß√£o
    print(f"Ganho m√°ximo em uma opera√ß√£o: R$ {max_gain:.2f}")
    # Exibe o preju√≠zo m√°ximo em uma √∫nica opera√ß√£o
    print(f"Preju√≠zo m√°ximo em uma opera√ß√£o: R$ {max_loss:.2f}\n")


    # =============================================================================
    # 22. Exibir Resumo Geral dos Resultados e Salvar CSV
    # =============================================================================
    # Inicia o c√°lculo do resumo geral dos resultados.
    print("Calculando resumo geral dos resultados...")

    # Calcula o total de lucro ou preju√≠zo gerado por todas as opera√ß√µes, somando os valores na coluna 'Lucro/Preju√≠zo'
    total_lucro = df["Lucro/Preju√≠zo"].sum()

    # Calcula o ROI (Retorno sobre Investimento), usando o total de lucro ou preju√≠zo e o capital inicial.
    # O ROI √© calculado como o lucro total dividido pelo capital inicial multiplicado por 100 para obter o valor percentual.
    roi = (total_lucro / capital_inicial) * 100

    # Calcula o capital final ap√≥s o lucro ou preju√≠zo, somando o capital inicial com o total de lucro ou preju√≠zo.
    capital_final = capital_inicial + total_lucro

    # Exibe o valor do capital inicial, com formata√ß√£o para mostrar o valor em reais (R$).
    print(f"\nüí∞ Capital Inicial: R$ {capital_inicial:,.2f}")

    # Exibe o valor total de lucro ou preju√≠zo gerado pelas opera√ß√µes.
    print(f"üìà Lucro/Preju√≠zo Total: R$ {total_lucro:.2f}")

    # Exibe o ROI calculado, indicando o retorno percentual do investimento.
    print(f"üìä ROI: {roi:.2f}%")

    # Exibe o capital final, ou seja, o capital inicial somado ao lucro ou preju√≠zo obtido.
    print(f"üîπ Capital Final: R$ {capital_final:.2f}\n")


# =============================================================================
# Loop principal
# =============================================================================
# Percorre todos os s√≠mbolos na lista 'symbols' e processa cada um deles
for symbol in symbols:
    try:
        # Inicia o processamento para o s√≠mbolo atual, imprimindo uma mensagem
        print(f"Processamento inciado: {symbol} !")

        # Chama a fun√ß√£o 'processar_acao' para realizar o processamento dos dados
        # para o s√≠mbolo atual (a fun√ß√£o 'processar_acao' √© respons√°vel por diversas opera√ß√µes no s√≠mbolo)
        processar_acao(symbol)

        # Ap√≥s a execu√ß√£o da fun√ß√£o 'processar_acao', indica que o processamento foi conclu√≠do com sucesso
        print(f"Processamento Conclu√≠do: {symbol} !")

    except Exception as e:
        # Se ocorrer algum erro ao processar o s√≠mbolo, captura a exce√ß√£o e imprime uma mensagem de erro
        print(f"Erro ao processar {symbol}: {e}")

# =============================================================================
# Salvar Excel consolidado
# =============================================================================
# Define o nome do arquivo Excel onde os resultados consolidados ser√£o salvos
excel_consolidado = "Resultados_Modelos.xlsx"

# Cria um DataFrame 'df_resultados' a partir da lista 'resultados_finais', que cont√©m os resultados de cada s√≠mbolo
df_resultados = pd.DataFrame(resultados_finais)

# Utiliza o ExcelWriter com o engine 'xlsxwriter' para salvar o DataFrame em um arquivo Excel
with pd.ExcelWriter(excel_consolidado, engine='xlsxwriter') as writer:
    # Salva os dados do DataFrame na planilha "Resumo_Modelos" do arquivo Excel
    # O par√¢metro 'index=False' indica que o √≠ndice n√£o ser√° inclu√≠do no arquivo Excel
    df_resultados.to_excel(writer, index=False, sheet_name="Resumo_Modelos")

# Ap√≥s o processo de salvar o Excel, imprime uma mensagem indicando o caminho do arquivo salvo
print(f"\n‚úÖ Resultados consolidados salvos em: {excel_consolidado}")

